<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Publications - Anna Rogers</title>
<meta name="description" content="Anna Rogers’ personal page">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Anna Rogers">
<meta property="og:title" content="Publications">
<meta property="og:url" content="http://localhost:4000/publications/">













<link rel="canonical" href="http://localhost:4000/publications/">













<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Anna Rogers Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<!--bibtex hack-->
<script>
function showBibtex(bibDiv) {
  var x = document.getElementById(bibDiv);
  if (x.style.display === "none" || x.style.display === '') {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>
    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!--link rel="icon" type="image/png" href="/assets/images/logo-3col.png"-->

<link href="https://fonts.googleapis.com/css?family=Roboto&display=swap" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Oswald&display=swap" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="/assets/css/academicons.min.css"/>
<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">Anna Rogers</a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/" >Home</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/" >Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/talks/" >Talks</a>
            </li><li class="masthead__menu-item">
              <a href="/service/" >Organizing</a>
            </li><li class="masthead__menu-item">
              <a href="/outreach/" >Outreach</a>
            </li><li class="masthead__menu-item">
              <a href="https://hackingsemantics.xyz/year-archive/" >Blog</a>
            </li><li class="masthead__menu-item">
              <a href="mailto:arogers@cs.uml.edu" >Contact</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      

      
        <img src="/assets/images/aro.jpg" alt="" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name"></h3>
    
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="http://sodas.ku.dk/" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-university" aria-hidden="true"></i> University of Copenhagen</a></li>
          
        
          
            <li><a href="https://scholar.google.com/citations?user=5oCYOE0AAAAJ&hl=en" rel="nofollow noopener noreferrer"><i class="ai ai-google-scholar ai" aria-hidden="true"></i> Google Scholar</a></li>
          
        
          
            <li><a href="https://www.semanticscholar.org/author/Anna-Rogers/145046059" rel="nofollow noopener noreferrer"><i class="ai ai-semantic-scholar ai" aria-hidden="true"></i> Semantic Scholar</a></li>
          
        
          
            <li><a href="https://orcid.org/0000-0002-4845-4023" rel="nofollow noopener noreferrer"><i class="ai ai-orcid ai" aria-hidden="true"></i> ORCID</a></li>
          
        
      


      

      

      

      
        <li>
          <a href="https://twitter.com/annargrs" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter
          </a>
        </li>
      

      

      
        <li>
          <a href="https://www.linkedin.com/in/annargrs" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/annargrs" itemprop="sameAs" rel="nofollow noopener noreferrer">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  


<!-- stolen from here:
https://www.gungorbudak.com/blog/2017/12/08/tags-cloud-sorted-by-post-count-for-jekyll-blogs-without-plugins/
-->
<!--div class="tag-cloud">








</div-->



  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Publications">
    
    
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Publications
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Total pubs: 21</p>

<h3 id="2020">2020</h3>

<ol class="bibliography"><li><div class="text-justify">
    <span id="PrasannaRogersEtAl_2020_When_BERT_Plays_Lottery_All_Tickets_Are_Winning">Prasanna, S., <b>Rogers, A.</b>, &amp; Rumshisky, A. (2020). When BERT Plays the Lottery, All Tickets Are Winning. <i>ArXiv:2005.00561 [Cs]</i>.</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('PrasannaRogersEtAl_2020_When_BERT')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://arxiv.org/abs/2005.00561'">PDF</button>
    

    

    

    

    

    

    
    <div class="abstract" id="PrasannaRogersEtAl_2020_When_BERT">Much of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the "good" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.</div>
    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersKovalevaEtAl_2020_Getting_Closer_to_AI_Complete_Question_Answering_Set_of_Prerequisite_Real_Tasks"><b>Rogers, A.</b>, Kovaleva, O., Downey, M., &amp; Rumshisky, A. (2020). Getting Closer to AI Complete Question Answering: A Set of Prerequisite Real Tasks. <i>Proceedings of the  AAAI Conference on Artificial Intelligence</i>, 11.</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('RogersKovalevaEtAl_2020_Getting_Closer')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://aaai.org/Papers/AAAI/2020GB/AAAI-RogersA.7778.pdf'">PDF</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://text-machine-lab.github.io/blog/2020/quail/'">BLOG</button>
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://text-machine.cs.uml.edu/lab2/projects/quail/'">DATA</button>
    

    

    

    
    <div class="abstract" id="RogersKovalevaEtAl_2020_Getting_Closer">The recent explosion in question answering research produced a wealth of both factoid RC and commonsense reasoning datasets. Combining them presents a different kind of task: not deciding simply whether information is present in the text, but also whether a confident guess could be made for the missing information. To that end, we present QuAIL, the first reading comprehension dataset (a) to combine textbased, world knowledge and unanswerable questions, and (b) to provide annotation that would enable precise diagnostics of the reasoning strategies by a given QA system. QuAIL contains 15K multi-choice questions for 800 texts in 4 domains (fiction, blogs, political news, and user story texts). Crucially, to solve QuAIL a system would need to handle both general and text-specific questions, impossible to answer from pretraining data. We show that the new benchmark poses substantial challenges to the current state-of-the-art systems, with a 30% drop in accuracy compared to the most similar existing dataset.</div>
    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersKovalevaEtAl_2020_Primer_in_BERTology_What_we_know_about_how_BERT_works"><b>Rogers, A.</b>, Kovaleva, O., &amp; Rumshisky, A. (2020). A Primer in BERTology: What We Know about How BERT Works. <i>(Accepted to TACL)</i>.</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('RogersKovalevaEtAl_2020_Primer_in')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://arxiv.org/abs/2002.12327'">PDF</button>
    

    

    

    

    

    

    
    <div class="abstract" id="RogersKovalevaEtAl_2020_Primer_in">Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research.</div>
    
</div>



    
</li></ol>

<h3 id="2019">2019</h3>

<ol class="bibliography"><li><div class="text-justify">
    <span id="KovalevaRomanovEtAl_2019_Revealing_Dark_Secrets_of_BERT">Kovaleva, O., Romanov, A., <b>Rogers, A.</b>, &amp; Rumshisky, A. (2019). Revealing the Dark Secrets of BERT. <i>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</i>, 4356–4365. https://doi.org/10.18653/v1/D19-1445</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('KovalevaRomanovEtAl_2019_Revealing_Dark')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://www.aclweb.org/anthology/D19-1445'">PDF</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://text-machine-lab.github.io/blog/2020/bert-secrets/'">BLOG</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://github.com/text-machine-lab/dark-secrets-of-BERT/'">CODE</button>
    

    

    

    

    
    <div class="abstract" id="KovalevaRomanovEtAl_2019_Revealing_Dark">BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.</div>
    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersDrozdEtAl_2019_Proceedings_of_3rd_Workshop_on_Evaluating_Vector_Space_Representations_for_NLP"><b>Rogers, A.</b>, Drozd, A., Rumshisky, A., &amp; Goldberg, Y. (2019). <i>Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</i>.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://www.aclweb.org/anthology/papers/W/W19/W19-2000/'">PDF</button>
    

    

    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersKovalevaEtAl_2019_Calls_to_Action_on_Social_Media_Potential_for_Censorship_and_Social_Impact"><b>Rogers, A.</b>, Kovaleva, O., &amp; Rumshisky, A. (2019). Calls to Action on Social Media: Potential for Censorship and Social Impact. <i>EMNLP-IJCNLP 2019 Second Workshop on Natural Language Processing for Internet Freedom</i>.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://www.aclweb.org/anthology/D19-5005'">PDF</button>
    

    

    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersSmelkovEtAl_2019_NarrativeTime_Dense_High-Speed_Temporal_Annotation_on_Timeline"><b>Rogers, A.</b>, Smelkov, G., &amp; Rumshisky, A. (2019). NarrativeTime: Dense High-Speed Temporal Annotation on a Timeline. <i>ArXiv:1908.11443 [Cs]</i>.</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('RogersSmelkovEtAl_2019_NarrativeTime_Dense')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://arxiv.org/abs/1908.11443'">PDF</button>
    

    

    

    

    

    

    
    <div class="abstract" id="RogersSmelkovEtAl_2019_NarrativeTime_Dense">We present NarrativeTime, a new timeline-based annotation scheme for temporal order of events in text, and a new densely annotated fiction corpus comparable to TimeBank-Dense. NarrativeTime is considerably faster than schemes based on event pairs such as TimeML, and it produces more temporal links between events than TimeBank-Dense, while maintaining comparable agreement on temporal links. This is achieved through new strategies for encoding vagueness in temporal relations and an annotation workflow that takes into account the annotators’ chunking and commonsense reasoning strategies. NarrativeTime comes with new specialized web-based tools for annotation and adjudication.</div>
    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RomanovRumshiskyEtAl_2019_Adversarial_Decomposition_of_Text_Representation">Romanov, A., Rumshisky, A., <b>Rogers, A.</b>, &amp; Donahue, D. (2019). Adversarial Decomposition of Text Representation. <i>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</i>, 815–825.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://aclweb.org/anthology/papers/N/N19/N19-1088/'">PDF</button>
    

    

    

    

    

    

    
</div>



    
</li></ol>

<h3 id="2018">2018</h3>

<ol class="bibliography"><li><div class="text-justify">
    <span id="KarpinskaLiEtAl_2018_Subcharacter_Information_in_Japanese_Embeddings_When_Is_It_Worth_It">Karpinska, M., Li, B., <b>Rogers, A.</b>, &amp; Drozd, A. (2018). Subcharacter Information in Japanese Embeddings: When Is It Worth It? <i>Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP</i>, 28–37. Melbourne, Australia: Association for Computational Linguistics.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://aclweb.org/anthology/W18-2905'">PDF</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/jBATS/'">BLOG</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/jBATS/'">CODE</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/jBATS/'">DATA</button>
    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersHosurAnanthakrishnaEtAl_2018_Whats_in_Your_Embedding_And_How_It_Predicts_Task_Performance"><b>Rogers, A.</b>, Hosur Ananthakrishna, S., &amp; Rumshisky, A. (2018). What’s in Your Embedding, And How It Predicts Task Performance. <i>Proceedings of the 27th International Conference on Computational Linguistics</i>, 2690–2703. Santa Fe, New Mexico, USA, August 20-26, 2018: Association for Computational Linguistics.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://aclweb.org/anthology/C18-1228'">PDF</button>
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://ldtoolkit.space'">CODE</button>
    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersRomanovEtAl_2018_RuSentiment_Enriched_Sentiment_Analysis_Dataset_for_Social_Media_in_Russian"><b>Rogers, A.</b>, Romanov, A., Rumshisky, A., Volkova, S., Gronas, M., &amp; Gribov, A. (2018). RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian. <i>Proceedings of the 27th International Conference on Computational Linguistics</i>, 755–763. Santa Fe, New Mexico, USA: Association for Computational Linguistics.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://aclweb.org/anthology/C18-1064'">PDF</button>
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://github.com/text-machine-lab/rusentiment_baselines'">CODE</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://text-machine.cs.uml.edu/lab2/projects/rusentiment/'">DATA</button>
    

    

    

    
</div>



    
</li></ol>

<h3 id="2017">2017</h3>

<ol class="bibliography"><li><div class="text-justify">
    <span id="LiLiuEtAl_2017_Investigating_different_syntactic_context_types_and_context_representations_for_learning_word_embeddings">Li, B., Liu, T., Zhao, Z., Tang, B., Drozd, A., <b>Rogers, A.</b>, &amp; Du, X. (2017). Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings. <i>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</i>, 2411–2421. Copenhagen, Denmark, September 7–11, 2017.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://aclweb.org/anthology/D17-1257'">PDF</button>
    

    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.readthedocs.io/en/docs/tutorial/getting_vectors.html#pre-trained-vsms'">DATA</button>
    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="Rogers_2017_Multilingual_computational_lexicography_frame_semantics_meets_distributional_semantics"><b>Rogers, A.</b> (2017). <i>Multilingual Computational Lexicography: Frame Semantics Meets Distributional Semantics</i> (Ph.D. Dissertation). University of Tokyo, Tokyo.</span>

    
    

    

    

    

    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="RogersDrozdEtAl_2017_Too_Many_Problems_of_Analogical_Reasoning_with_Word_Vectors"><b>Rogers, A.</b>, Drozd, A., &amp; Li, B. (2017). The (Too Many) Problems of Analogical Reasoning with Word Vectors. <i>Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017)</i>, 135–148.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://www.aclweb.org/anthology/S17-1017'">PDF</button>
    

    

    

    

    

    

    
</div>



    
</li></ol>

<h3 id="2016">2016</h3>

<p><em>(before 2017 my last name was “Gladkova”)</em></p>

<ol class="bibliography"><li><div class="text-justify">
    <span id="DrozdGladkovaEtAl_2016_Word_embeddings_analogies_and_machine_learning_beyond_king_-_man_woman_queen">Drozd, A., <b>Gladkova, A.</b>, &amp; Matsuoka, S. (2016). Word Embeddings, Analogies, and Machine Learning: Beyond King - Man + Woman = Queen. <i>Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</i>, 3519–3530. Osaka, Japan, December 11-17.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://www.aclweb.org/anthology/C/C16/C16-1332.pdf'">PDF</button>
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://github.com/vecto-ai/vecto/'">CODE</button>
    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="GladkovaDrozd_2016_Intrinsic_evaluations_of_word_embeddngs_what_can_we_do_better"><b>Gladkova, A.</b>, &amp; Drozd, A. (2016). Intrinsic Evaluations of Word Embeddings: What Can We Do Better? <i>Proceedings of The 1st Workshop on Evaluating Vector Space Representations for NLP</i>, 36–42. https://doi.org/10.18653/v1/W16-2507</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://www.aclweb.org/anthology/W/W16/W16-2507.pdf'">PDF</button>
    

    

    

    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="GladkovaDrozdEtAl_2016_Analogy-based_detection_of_morphological_and_semantic_relations_with_word_embeddings_what_works_and_what_doesnt"><b>Gladkova, A.</b>, Drozd, A., &amp; Matsuoka, S. (2016). Analogy-Based Detection of Morphological and Semantic Relations with Word Embeddings: What Works and What Doesn’t. <i>Proceedings of the NAACL-HLT SRW</i>, 47–54. https://doi.org/10.18653/v1/N16-2002</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://www.aclweb.org/anthology/N/N16/N16-2002.pdf'">PDF</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/BATS/'">BLOG</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/BATS/'">CODE</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'https://vecto.space/projects/BATS/'">DATA</button>
    

    

    

    
</div>



    
</li>
<li><div class="text-justify">
    <span id="SantusGladkovaEtAl_2016_CogALex-V_shared_task_on_corpus-based_identification_of_semantic_relations">Santus, E., <b>Gladkova, A.</b>, Evert, S., &amp; Lenci, A. (2016). The CogALex-V Shared Task on the Corpus-Based Identification of Semantic Relations. <i>Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V)</i>, 69–79. Osaka, Japan, December 11-17: ACL.</span>

    
    

    

    
    <button class="btn--success" onclick="window.location.href = 'http://www.aclweb.org/anthology/W/W16/W16-53.pdf#page=83'">PDF</button>
    

    

    

    
    <button class="btn--success" onclick="window.location.href = 'https://sites.google.com/site/cogalex2016/home/shared-task'">DATA</button>
    

    

    

    
</div>



    
</li></ol>

<h3 id="2015">2015</h3>

<ol class="bibliography"><li><div class="text-justify">
    <span id="DrozdGladkovaEtAl_2015_Discovering_aspectual_classes_of_Russian_verbs_in_untagged_large_corpora">Drozd, A., <b>Gladkova, A.</b>, &amp; Matsuoka, S. (2015). Discovering Aspectual Classes of Russian Verbs in Untagged Large Corpora. <i>Proceedings of 2015 IEEE International Conference on Data Science and Data Intensive Systems (DSDIS)</i>, 61–68. https://doi.org/10.1109/DSDIS.2015.30</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('DrozdGladkovaEtAl_2015_Discovering_aspectual')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://www.ieeexplore.ieee.org/document/7396482/'">PDF</button>
    

    

    

    

    

    

    
    <div class="abstract" id="DrozdGladkovaEtAl_2015_Discovering_aspectual">This paper presents a case study of discovering and classifying verbs in large web-corpora. Many tasks in natural language processing require corpora containing billions of words, and with such volumes of data co-occurrence extraction becomes one of the performance bottlenecks in the Vector Space Models of computational linguistics. We propose a co-occurrence extraction kernel based on ternary trees as an alternative (or a complimentary stage) to conventional map-reduce based approach, this kernel achieves an order of magnitude improvement in memory footprint and processing speed. Our classifier successfully and efficiently identified verbs in a 1.2-billion words untagged corpus of Russian fiction and distinguished between their two aspectual classes. The model proved efficient even for low-frequency vocabulary, including nonce verbs and neologisms.</div>
    
</div>



    
</li>
<li><div class="text-justify">
    <span id="DrozdGladkovaEtAl_2015_Python_performance_and_Natural_Language_Processing">Drozd, A., <b>Gladkova, A.</b>, &amp; Matsuoka, S. (2015). Python, Performance, and Natural Language Processing. <i>Proceedings of the 5th Workshop on Python for High-Performance and Scientific Computing</i>, 1:1–1:10. https://doi.org/10.1145/2835857.2835858</span>

    
    

    
    <button class="btn--info" onclick="showBibtex('DrozdGladkovaEtAl_2015_Python_performance')">Abstract</button>
    

    
    <button class="btn--success" onclick="window.location.href = 'http://dl.acm.org/citation.cfm?id=2835858'">PDF</button>
    

    

    

    

    

    

    
    <div class="abstract" id="DrozdGladkovaEtAl_2015_Python_performance">We present a case study of Python-based workflow for a data-intensive natural language processing problem, namely word classification with vector space model methodology. Problems in the area of natural language processing are typically solved in many steps which require transformation of the data to vastly different formats (in our case, raw text to sparse matrices to dense vectors). A Python implementation for each of these steps would require a different solution. We survey existing approaches to using Python for high-performance processing of large volumes of data, and we propose a sample solution for each step for our case study (aspectual classification of Russian verbs), attempting to preserve both efficiency and user-friendliness. For the most computationally intensive part of the workflow we develop a prototype distributed implementation of co-occurrence extraction module using IPython.parallel cluster.</div>
    
</div>



    
</li></ol>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    <!--li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li-->
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Anna Rogers. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.
  <!--a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/80x15.png" /></a-->This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.
</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>













  </body>
</html>
